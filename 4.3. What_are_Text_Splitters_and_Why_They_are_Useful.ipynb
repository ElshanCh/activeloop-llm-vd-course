{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "Large Language Models, while recognized for creating human-like text, can also \"hallucinate\" and produce seemingly plausible yet incorrect or nonsensical information. Interestingly, this tendency can be advantageous in creative tasks, as it generates a range of unique and imaginative ideas, sparking new perspectives and driving the creative process. However, this poses a challenge in situations where accuracy is critical, such as code reviews, insurance-related tasks, or research question responses.\n",
    "\n",
    "One approach to mitigating hallucination is to provide documents as sources of information to the LLM and ask it to generate an answer based on the knowledge extracted from the document. This can help reduce the likelihood of hallucination, and users can verify the information with the source document.\n",
    "\n",
    "Let's discuss the pros and cons of this approach:\n",
    "\n",
    "#### **Pros:**\n",
    "- Reduced hallucination: By providing a source document, the LLM is more likely to generate content based on the given information, reducing the chances of creating false or irrelevant information.\n",
    "- Increased accuracy: With a reliable source document, the LLM can generate more accurate answers, especially in use cases where accuracy is crucial.\n",
    "- Verifiable information: Users can cross-check the generated content with the source document to ensure the information is accurate and reliable.\n",
    "\n",
    "#### **Cons:**\n",
    "- Limited scope: Relying on a single document may limit the scope of the generated content, as the LLM will only have access to the information provided in the document.\n",
    "- Dependence on document quality: The accuracy of the generated content heavily depends on the quality and reliability of the source document. The LLM will likely generate incorrect or misleading content if the document contains inaccurate or biased information.\n",
    "- Inability to eliminate hallucination completely: Although providing a document as a base reduces the chances of hallucination, it does not guarantee that the LLM will never generate false or irrelevant information.\n",
    "\n",
    "Addressing another challenge, LLMs have a maximum prompt size, preventing them from feeding entire documents. This makes it crucial to divide documents into smaller parts, and Text Splitters prove to be extremely useful in achieving this. Text Splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively.\n",
    "\n",
    "Using a Text Splitter can also improve vector store search results, as smaller segments might be more likely to match a query. Experimenting with different chunk sizes and overlaps can be beneficial in tailoring results to suit your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Customizing Text Splitter**\n",
    "When handling lengthy pieces of text, it's crucial to break them down into manageable chunks. This seemingly simple task can quickly become complex, as keeping semantically related text segments intact is essential. The definition of \"semantically related\" may vary depending on the type of text. In this article, we'll explore various strategies to achieve this.\n",
    "\n",
    "At a high level, text splitters follow these steps:\n",
    "\n",
    "1. Divide the text into small, semantically meaningful chunks (often sentences).\n",
    "2. Combine these small chunks into a larger one until a specific size is reached (determined by a particular function).\n",
    "3. Once the desired size is attained, separate that chunk as an individual piece of text, then start forming a new chunk with some overlap to maintain context between segments.\n",
    "\n",
    "Consequently, there are two primary dimensions to consider when customizing your text splitter:\n",
    "\n",
    "1. The method used to split the text\n",
    "2. The approach for measuring chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Character Text Splitter**\n",
    "This type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks. For example, you might use it to split a long article into smaller chunks for easier processing or analysis. The splitter allows you to customize the chunking process along two axes - chunk size and chunk overlap - to balance the trade-offs between splitting the text into manageable pieces and preserving semantic context between chunks.\n",
    "\n",
    "To load the documents using the `PyPDFLoader` class, you need to install the `pypdf` package using Python Package Manager. Run the following command to install it: \n",
    "\n",
    "```\n",
    "pip install -q pypdf\n",
    "```\n",
    "\n",
    "Remember to also install the required packages with the following command:\n",
    "\n",
    "```\n",
    "pip install langchain==0.0.208 deeplake openai tiktoken\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Deep_Learning_for_Natural_Language_Processing.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading the text file, we can ask more specific questions related to the subject, which helps minimize the likelihood of LLM hallucinations and ensures more accurate, context-driven responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Deep Learning for \\nNatural Language Processing\\nDevelop Deep Learning Models for \\nNatural Language in Python\\nJason Brownlee' metadata={'source': 'Deep_Learning_for_Natural_Language_Processing.pdf', 'page': 0}\n",
      "You have 414 documents\n",
      "Preview:\n",
      "Deep Learning for \n",
      "Natural Language Processing\n",
      "Develop Deep Learning Models for \n",
      "Natural Language in Python\n",
      "Jason Brownlee\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No universal approach for chunking text will fit all scenarios - what's effective for one case might not be suitable for another. Finding the best chunk size for your project means going through a few steps. \n",
    "- First, clean up your data by getting rid of anything that's not needed, like HTML tags from websites. \n",
    "- Then, pick a few different chunk sizes to test. The best size will depend on what kind of data you're working with and the model you're using.  \n",
    "- Finally, test out how well each size works by running some queries and comparing the results. You might need to try a few different sizes before finding the best one. This process might take some time, but getting the best results from your project is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recursive Character Text Splitter**\n",
    "The Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in order until the resulting chunks are small enough. By default, the list of characters used for splitting is `[\"\\n\\n\", \"\\n\", \" \"]`, which tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text. This means that the class first tries to split the text into chunks using two new-line characters (`\"\\n\\n\"`). If the resulting chunks are still larger than the desired chunk size, it will then try to split the output by a single new-line character (`\"\\n\"`), followed by a space character (`\" \"`), and so on, until the desired chunk size is achieved.\n",
    "\n",
    "To use the RecursiveCharacterTextSplitter, you can create an instance of it and provide the following parameters:\n",
    "\n",
    "- `chunk_size`: The maximum size of the chunks, as measured by the `length_function` (default is 100).\n",
    "- `chunk_overlap`: The maximum overlap between chunks to maintain continuity between them (default is 20).\n",
    "- `length_function`: This parameter is used to calculate the length of the chunks. By default, it is set to `len`, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n",
    "\n",
    "Using a token counter instead of the default `len` function can benefit specific scenarios, such as when working with language models with token limits. For example, OpenAI's GPT-3 has a token limit of 4096 tokens per request, so you might want to count tokens instead of characters to better manage and optimize your requests.\n",
    "\n",
    "Here's an example of how to use RecursiveCharacterTextSplitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('my_file.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the `RecursiveCharacterTextSplitter` class with the desired parameters. The default list of characters to split by is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is first split by two new-line characters (`\\n\\n`). Then, since the chunks are still larger than the desired chunk size (50), the class tries to split the output by a single new-line character (`\\n`). The final output consists of two chunks, each with a length of 50 characters or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3', metadata={}),\n",
       " Document(page_content='Google is offering developers access to one of its most advanced AI language models: PaLM.', metadata={}),\n",
       " Document(page_content='The search giant is launching an API for PaLM alongside a number of AI enterprise tools', metadata={}),\n",
       " Document(page_content='it says will help businesses \\x93generate text, images, code, videos, audio, and more from', metadata={}),\n",
       " Document(page_content='simple natural language prompts.\\x94', metadata={}),\n",
       " Document(page_content='PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or', metadata={}),\n",
       " Document(page_content='Meta\\x92s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,', metadata={}),\n",
       " Document(page_content='PaLM is a flexible system that can potentially carry out all sorts of text generation and', metadata={}),\n",
       " Document(page_content='editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for', metadata={}),\n",
       " Document(page_content='example, or you could use it for tasks like summarizing text or even writing code.', metadata={}),\n",
       " Document(page_content='(It\\x92s similar to features Google also announced today for its Workspace apps like Google', metadata={}),\n",
       " Document(page_content='Docs and Gmail.)', metadata={})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([sample_text])\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the text is loaded from a file, and the `RecursiveCharacterTextSplitter` is used to split it into chunks with a maximum size of 50 characters and an overlap of 10 characters. The output will be a list of documents containing the split text.\n",
    "\n",
    "To use a token counter, you can create a custom function that calculates the number of tokens in a given text and pass it as the `length_function` parameter. This will ensure that your text splitter calculates the length of chunks based on the number of tokens instead of the number of characters. The exploration of this concept will be part of our upcoming lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLTK Text Splitter**\n",
    "The `NLTKTextSplitter` in LangChain is an implementation of a text splitter that uses the Natural Language Toolkit (NLTK) library to split text based on tokenizers. The goal is to split long texts into smaller chunks without breaking the structure of sentences and paragraphs.\n",
    "\n",
    "ðŸ’¡\n",
    "If it is your first time using this package, it is required to install the `NLTK` library using `pip install -q nltk` and run the following Python code to download the packages that LangChain needs.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\n\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses \\x93generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.\\x94\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta\\x92s LLaMA family of models.',\n",
       " 'Google first announced PaLM in April 2022.\\n\\nLike other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks.\\n\\nYou could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n\\n(It\\x92s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a long document\n",
    "with open('my_file.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as mentioned in your context, the NLTKTextSplitter is not specifically designed to handle word segmentation in English sentences without spaces. For this purpose, you can use alternative libraries like pyenchant or word segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SpacyTextSplitter**\n",
    "The `SpacyTextSplitter` helps split large text documents into smaller chunks based on a specified size. This is useful for better management of large text inputs. It's important to note that the `SpacyTextSplitter` is an alternative to NLTK-based sentence splitting. You can create a `SpacyTextSplitter` object by specifying the `chunk_size` parameter, measured by a length function passed to it, which defaults to the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
      "\n",
      "\n",
      "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
      "\n",
      "\n",
      "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
      "it says will help businesses Â“generate text, images, code, videos, audio, and more from\n",
      "simple natural language prompts.Â”\n",
      "\n",
      "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
      "MetaÂ’s LLaMA family of models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "\n",
    "# Load a long document\n",
    "with open('my_file.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "\n",
    "# Print the first chunk\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MarkdownTextSplitter**\n",
    "The `MarkdownTextSplitter` is designed to split text written using Markdown languages like headers, code blocks, or dividers. It is implemented as a simple subclass of `RecursiveCharacterSplitter` with Markdown-specific separators. By default, these separators are determined by the Markdown syntax, but they can be customized by providing a list of characters during the initialization of the `MarkdownTextSplitter` instance. The chunk size, which is initially set to the number of characters, is measured by the length function passed in. To customize the chunk size, provide an integer value when initializing an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# \\n\\n# Welcome to My Blog!', metadata={}),\n",
       " Document(page_content='Introduction', metadata={}),\n",
       " Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}),\n",
       " Document(page_content='Java, and JavaScript.', metadata={}),\n",
       " Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}),\n",
       " Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}),\n",
       " Document(page_content='About this Blog', metadata={}),\n",
       " Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}),\n",
       " Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}),\n",
       " Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}),\n",
       " Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}),\n",
       " Document(page_content='Stay tuned for more updates!', metadata={}),\n",
       " Document(page_content='Contact Me', metadata={}),\n",
       " Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}),\n",
       " Document(page_content='johndoe@email.com.', metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "markdown_text = \"\"\"\n",
    "# \n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MarkdownTextSplitter offers a practical solution for dividing text while preserving the structure and meaning provided by Markdown formatting. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks. This splitter is especially valuable when managing extensive Markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TokenTextSplitter**\n",
    "The main advantage of using `TokenTextSplitter` over other text splitters, like `CharacterTextSplitter`, is that it respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This can be particularly helpful in maintaining the semantic integrity of the text when working with language models and embeddings.\n",
    "\n",
    "This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks. It then reassembles the tokens within each chunk back into text. The `tiktoken` python package is required for using this class. (`pip install -q tiktoken`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses \\x93generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.\\x94\\n\\nPaLM is a large language model, or LLM,',\n",
       " ' is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta\\x92s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summar',\n",
       " ', or you could use it for tasks like summarizing text or even writing code.\\n(It\\x92s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('my_file.txt', encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chunk_size` parameter sets the maximum number of BPE tokens in each chunk, while `chunk_overlap` defines the number of overlapping tokens between adjacent chunks. By modifying these parameters, you can fine-tune the granularity of the text chunks.\n",
    "\n",
    "One potential drawback of using `TokenTextSplitter` is that it may require additional computation when converting text to BPE tokens and back. If you need a faster and simpler text-splitting method, you might consider using `CharacterTextSplitter`, which directly splits the text based on character count, offering a more straightforward approach to text segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RECAP:**\n",
    "\n",
    "Text splitters are essential for managing long text, improving language model processing efficiency, and enhancing vector store search results. Customizing text splitters involves selecting the splitting method and measuring chunk size. \n",
    "\n",
    "- `CharacterTextSplitter` is an example that helps balance manageable pieces and semantic context preservation. Experimenting with different chunk sizes and overlaps tailors the results for specific use cases.\n",
    "\n",
    "- `RecursiveCharacterTextSplitter` focuses on preserving semantic relationships while offering customizable chunk sizes and overlaps.\n",
    "\n",
    "- `NLTKTextSplitter` utilizes the Natural Language Toolkit library for more accurate text segmentation. \n",
    "\n",
    "- `SpacyTextSplitter` leverages the popular SpaCy library to split texts based on linguistic features.\n",
    "\n",
    "- `MarkdownTextSplitter` is tailored for Markdown-formatted texts, ensuring content is split meaningfully according to the syntax.\n",
    "\n",
    "- Lastly, `TokenTextSplitter` employs BPE tokens for splitting, offering a fine-grained approach to text segmentation.\n",
    "\n",
    "## **Conclusion**\n",
    "Selecting the appropriate text splitter depends on the specific requirements and nature of the text you are working with, ensuring optimal results for your text processing tasks.\n",
    "\n",
    "In the next lesson, weâ€™ll learn more about how word embeddings work and how embedding models are used with indexers in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RESOURCES:**\n",
    "-   [Split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)\n",
    "-   [Split code](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter)\n",
    "-   [Recursively split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activeloop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
